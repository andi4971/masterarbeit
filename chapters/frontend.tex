\chapter{Methods and Tools for Compiler Frontends}

In this chapter the methods and tools that are available for the construction of compiler frontends are explained. This explanation is focused on the parser generator ANTLR. The basis for this chapter is the book "The Definitive ANTLR 4 Reference" by \textcite{Antlr4Reference}.

\section{Attributed Grammar}

Using parser generators like ANTLR or Coco-2 requires the definition of the grammar in a specific format. These formats also allow for the declaration of semantic actions in the grammar. Semantic actions have access to the symbols (terminal and non-terminal) of a rule. Each symbol then has attributes associated with it. The combination of a grammar, attributes and semantic actions is called an attributed grammar.  


There are two types of attributes. Inherited and synthesized attributes. The former ones are computed based on the attributes of the parent node. Synthesized attributes are based on the attributes of the children nodes.  
The type of attributes available depends on the parsing strategy. For a top-down strategy the attributes of child-nodes are not available, as they have not been parsed yet. Conversely, when using the bottom-up strategy, the attributes of parent nodes are not available. 

Especially relevant are the attributes of terminal classes. Through the attribute of a terminal class like \texttt{number}, the actual number that this class node holds can be accessed. These kinds of attributes are provided by the lexical analyzer. 

In listing \ref{lst:Coco2ATG} a simple attributed grammar in Coco-2 for arithmetic expressions is shown. This grammar uses semantic actions to calculate the result of an arithmetic expression. Semantic actions are encoded inside \texttt{<< >>} blocks. In this case C\verb|#| code. Synthesized attributes provide the results of the calculations from the child nodes. These attributes are available inside the semantic action where the actual calculation is performed. 

While it is convenient to embed semantic actions directly into the grammar, it is not without disadvantages. By embedding code of a specific language, it is no longer possible to use the same grammar to generate a parser in another language. Parser generators like ANTLR provide multiple different target languages to generate a parser for. 

\begin{GenericCode}[float,numbers=none,caption=Attributed Grammar in Coco-2 for simple arithmetic expressions., label=lst:Coco2ATG]
Expr<<out int e>> =    LOCAL<<int t = 0; e = 0;>>
Term<<out e>>             
{ '+' Term<<out t>>    SEM<<e = e + t;>>
| '-' Term<<out t>>    SEM<<e = e - t;>>
}.

Term<<out int t>> =    LOCAL<<int f = 0; t = 0;>>
  Fact<<out t>>
  { '*' Fact<<out f>>  SEM<<t = t * f;>>
  | '/' Fact<<out f>>  SEM<<t = t / f;>>
  }.
  
Fact<<out int f>> =    LOCAL <<f = 0;>>
    number<<out f>>
  | '(' Expr<<out f>> ')'.

\end{GenericCode}

\section{ANTLR}

In this section the parser generator ANTLR (ANother Tool for Language Recognition) is explained. First the history of ANLR will be explained, followed by the introduction of the parsing algorithm currently employed by ANTLR, namely ALL(*). Finally, the general functionality of ANTLR is highlighted.  

\subsection{History}

"ANTLR (ANother Tool for Language Recognition) is a powerful parser generator for reading, processing, executing, or translating structured text or binary files". As the acronym of ANTLR states, it is a tool for language recognition. ANTLR was first released in 1992 and has since then been in continuous development. The original creator and maintainer of the project is Terence Parr. ANTLR is written in Java and is open source under the BSD license. Its source code can be viewed on GitHub\footnote{https://github.com/antlr/antlr4}. 

Many projects utilize ANTLR in their work. Notable examples include the Java Object-Relational Mapping tool \cite{HibernateWeb2024} and the NoSQL database Apache \textcite{Cassandra2024}.

ANTLR originally started of as the master thesis of Terrence Parr \parencite{PCCTSHistory1994}. A first alpha release was created in 1990, that only generated LL(1) parsers. Version 1 of ANTLR incorporated the new parsing algorithm developed by Parr that allowed to create parsers for LL(k) grammars \parencite{parrPhd1993}. Version2 then provided incremental improvements.   

Version 3, released in 2006 introduced a new parsing algorithm called LL(*) \parencite{LLSParsing2011}. The LL(*) parsing strategy performs parsing decisions at parse-time with a dynamic lookahead. The number of lookahead tokens increases to an arbitrary amount and falls back down using backtracking. However, a maximum amount of $k$ lookahead tokens still needed to be specified. Version 3 also introduced ANTLRWorks\footnote{https://www.antlr3.org/works}, a graphical IDE for the construction of ANTLR grammars.

The current version 4, released in 2013 again introduced a new parsing algorithm adaptive-LL(*) or ALL(*). The most significant improvement of ALL(*) over LL(*) is that the number of lookahead tokens no longer need to be specified manually. ALL(*) will be explained in more detail in section \ref{sec:allstar}. ANTLR v4 added support for the visitor and listener patterns, enabling easier interaction with the parsed data. 

\subsection{Parsing Algorithm Adaptive-LL(*)}
\label{sec:allstar}

The Adaptive-LL(*) or ALL(*) parsing strategy is introduced in the paper "Adaptive LL(*) parsing: the power of dynamic analysis" by \textcite{ALLParsing2014} and is the basis for this section. This parsing algorithm is the basis for ANTLR version 4. As the title suggests, ALL(*) performs the analysis of the grammar at parse time. 

\subsubsection{Limitations of LL(*) Parsing Algorithm}

To understand the need for ALL(*) it is necessary to highlight why the previous strategy LL(*) is insufficient. LL(*), introduced by \textcite{parr2011ll}, was developed as an improvement to the existing GLL \parencite{GLL2010} and GLR \parencite{tomita1991generalized} parsers.  For ambiguous grammars these parsers return multiple parse trees, which are undesired for parsers of programming languages. This is due to them being designed for natural languages, which are inherently ambiguous. LL(*) overcomes these limitations by using regular expressions that are stored inside a deterministic finite automaton (DFA) to offer mostly deterministic parsing. Using the DFA allows for regular lookahead even though the grammar itself is context-free. 

However, the LL(*) grammar condition cannot be checked statically, leading to the case that sometimes no regular expression is found that distinguishes the possible productions. Such situations are detected by the static analysis and then backtracking is used instead. Backtracking however comes with the disadvantage that for rules in the format $A \rightarrow a | ab$, the second alternative will never be matched, since backtracking always chooses the first alternative. 

\subsubsection{Dynamic Grammar Analysis with ALL(*)}

With ANTLR version 4 the parsing strategy Adaptive-LL(*) or ALL(*) was introduced. The main difference to ANTLR version 3 is that the grammar analysis is now performed at parse-time, and is no longer static. This overcomes the limitations of the static analysis LL(*) performs and enables the generation of correct parsers context-free grammars. The only exception are grammars that contain indirect or hidden left-recursion\footnote{Indirect left-recursion is a rule like $A \rightarrow B, B \rightarrow A$. $\epsilon$ productions cause hidden left-recursion. Take a rule $B \rightarrow \epsilon$ that produces only the empty chain $\epsilon$ and another rule $A \rightarrow BA$. Since B's only production is to $\epsilon$ the second rule causes a left-recursion. }. From an engineering perspective this was seen to be too much effort, since these grammars are deemed to be not common. Direct left-recursion is possible, because ANTLR rewrites the grammar to be non-direct left-recursive before passing it to the ALL(*) parsing algorithm. 

At a decision point(a rule containing multiple alternatives), ALL(*) starts a subparser for each alternative in pseudo-parallel. A subparser tries to match the remaining input to the selected alternative. If the input does not match, the subparser dies off. All subparsers process one symbol at the time in pseudo-parallel. This guarantees that the correct alternative can be found with minimum lookahead. In the case of ambiguity due to multiple subparsers reaching the end of file or coalescing, the first alternative will be chosen. 

The performance of ALL(*) is improved by employing a cache. This cache is implemented in the form of a DFA. The DFA stores the same information as the DFA generated by LL(*) from static analysis. After a lookahead the DFA stores which production resulted from the lookahead phrase. If at a later time the same lookahead phrase is being processed, the correct production can be retrieved from the DFA. Theoretically a DFA is not able to recognize a context-free grammar, however due to the analysis being performed at parse time, the analysis only needs to be performed on the remaining input. Since the remaining input is a subset of the context making it regular. Another optimization is the usage of a graph-structured stack (GSS). The GSS makes sure that during the prediction, no computation is performed twice, effectively acting as a cache. 

%In some cases it is necessary for ALL(*) to examine the parse stack to make a decision. 
%section about SLL
The theoretical runtime complexity of ALL(*) is $O(n^4)$. This stems from the fact that in the worst case the ALL(*) parser needs to make a prediction for each symbol and each launched subparser then needs to inspect the entire remaining input. 


\subsection{Functionality}

ANTLR generates a combined lexer and parser from a single grammar file. The generated parser is a recursive descent parser. ANTLR supports various target/host languages such as Java, C\verb|#| or C++. The syntax used by the ANTLR grammar is similar to \textit{yacc} and supports extended BNF (EBNF) operators such as \texttt{?}. To interact with the generated parser ANTLR optionally generates interfaces and implementations for the listener and visitor pattern. The usage of these patterns is explained in section XXX and XXX respectively.

ANTLR does not have a separate lexical analysis phase. Instead lexical and syntactical analysis are integrated into a unified process. Lexical rules are treated as parser rules, therefore a separate lexical analysis phase is not needed. Since the phases are combined, it is possible for ANTLR to perform context-sensitive lexing. The lexer can make a decision based on the current parsing context. The parsing is directly performed on the raw input stream and not on a separate token stream. 

\subsubsection{Semantic Predicates}

ANTLR supports the definition of so-called \textit{semantic predicates}. Semantic predicates are boolean expressions, defined in the host language that allow for the dynamic alteration of the language generated by the grammar. If a semantic predicate is present for a production, the production can only be accepted if the semantic predicate evaluates to true. Semantic predicates are expressed inside \verb|{ }| parenthesis followed by $?$. Listing \ref{lst:ANTLRSemPred} illustrates an example use case of a semantic predicate. The rule \texttt{blockEnd} contains a semantic predicate specifying that the production can only be accepted if there is currently a block on the stack. 

A semantic predicate also has access to the current token. This enables conditions that directly interact with the input. For example separate productions for even and uneven numbers could be used. The semantic predicate then checks if the number is even or not.  


\subsubsection{Semantic Actions}

In ANTLR grammars semantic actions can be defined. Semantic actions can be inserted in every parser rule, before, in between and after symbols. Similar to semantic predicates, the semantic action is to be defined in the host language. Semantic actions are expressed inside \verb|{ }| parenthesis. To access a symbol the name of the symbol prefixed by \verb|$| can be used. In Listing \ref{lst:ANTLRSemPred} semantic actions are used in the \texttt{blockStart} \texttt{blockEnd} rules. For the \texttt{blockEnd} it has to be noted that semantic predicates and actions can be used together. 

% semantic actions

\begin{GenericCode}[float,numbers=none,caption=Example grammar using a semantic predicate and a semantic action., label=lst:ANTLRSemPred]
grammar Example;
@members {
    java.util.Stack<String> blockStack = new java.util.Stack<>();
}

program: statement* EOF;

statement
    : blockStart
    | blockEnd
    | otherStatement
    ;

blockStart: 'begin' { blockStack.push("block"); };

blockEnd: 'end' { !blockStack.isEmpty() }? { blockStack.pop(); };

otherStatement: 'print' IDENTIFIER;

IDENTIFIER: [a-zA-Z_][a-zA-Z_0-9]*;

WS  : [ \t\r\n]+ -> skip;
\end{GenericCode}


\subsubsection{Alternative Labels for Rule Alternatives}

Per default ANTLR generates one method for each rule. In the case of multiple alternatives for a rule, the handling of the alternatives would need to be done manually. Therefore, ANTLR offers the possibility to attach a label to each of the alternatives. Then a method for each alternative will be generated separately. One use case is highlighted in listing \ref{lst:ANTLRRuleAlt}. The rule \texttt{type} matches to either one of the four types. Each alternative has a label associated to it by using \verb|#| as the prefix for the alternative name. With this four methods will be generating corresponding to each of the alternatives.     

\begin{GenericCode}[float,numbers=none,caption=Example rule using alternative labels for the rule alternatives., label=lst:ANTLRRuleAlt]
  type
      : 'int'     #IntType
      | 'bool'    #BoolType
      | 'long'    #LongType
      | 'string'  #StringType
      ;
  \end{GenericCode}

\section{Syntax Tree and Abstract Syntax Tree (AST)}

A syntax tree is a hierarchical representation of the syntactical structure of a sentence. Also referred to as a parse tree, this representation is usually generated by a parser. A syntax tree contains the information of the entire sentence based on the grammar of that language. Each node in the syntax tree responds to a rule in the grammar. The leaf nodes represent terminal symbols and non-leaf nodes are non-terminal symbols. Concatenating the leaf nodes from left to right of the syntax tree results in the original sentence from which the syntax tree was constructed from. 

Listing \ref{lst:SyntaxTreeEx} shows the syntax tree of the arithmetic expression \texttt{5 * 3 + 7}. This syntax tree contains the non-terminal symbol \texttt{Expression} and the terminal class \texttt{Number}. Each expression is built from two operands and one operator. An operand can either be another expression or a number. This structure further enables the representation of the precedence rules of arithmetic operations directly in the syntax tree.  

\subsubsection{Abstract Syntax Tree (AST)}
In an abstract syntax tree (AST) only a subset of the nodes from the original syntax tree are included. The goal is to focus on the semantic and structural aspects of the syntax tree. Syntactical details, e.g. the semicolon are omitted. 

The generation of an AST from a syntax tree can be done in multiple ways. One method is to generate the AST during the parse, at the same time as the full syntax tree is being built. This can be implemented by using an attributed grammar with semantic actions that embed the AST generation code directly into the parser. Parsers like ANTLR also support the listener pattern to execute code during the parse. Alternatively the AST can be generated after the parse phase from the syntax tree. To traverse the syntax tree the visitor pattern can be used for example.  

The AST is then used in the subsequent stages of a compiler. This transformation is performed to remove all information from the syntax tree that is of no importance to the following stages of the compiler. Subsequent code optimization may further slim down the AST. 

Continuing with the previous example, listing \ref{lst:ASTEx} shows the AST of the arithmetic expression \texttt{5 * 3 + 7}. This AST still contains the same semantic meaning as the full syntax tree, however it needs fewer nodes for that. Instead of using the non-terminal symbol \texttt{Expression}, the operator is used instead, effectively encoding the same information. In this example the node count can be halved from ten to five.

\begin{GenericCode}[float,numbers=none,caption=Syntax tree of the arithmetic expression \texttt{5 * 3 + 7}., label=lst:SyntaxTreeEx]
                                   Expression
                                  /     |    \
                            Expression  +    Number
                           /    |    \         |    
                        Number  *  Number      7
                          |          | 
                          5          3   
\end{GenericCode}

\begin{GenericCode}[float,numbers=none,caption=Abstract syntax tree of the arithmetic expression \texttt{5 * 3 + 7}., label=lst:ASTEx]
                                      +
                                    /   \
                                   *     7
                                 /   \
                                3     5
\end{GenericCode}

\subsection{Visitor-Pattern for Tree Transformation}

In the case that a syntax/parse tree is already present, the visitor-pattern can be used to transform it into an AST. Using the visitor-pattern, the syntax tree gets traversed and then step by step the AST is constructed. The visitor-pattern allows for the separation of algorithms from the objects they operate on. Instead of including the code for the generation of an AST object in the syntax tree object, a separate object, a so-called \textit{visitor} is taking care of this. 

To implement visitor-pattern for a syntax tree, the best approach is to use interfaces or abstract classes for the nodes of the syntax tree and the visitors. Listing \ref{lst:ListPatIntEx} shows a possible implementation for an interface and abstract class in Kotlin. This code is based on the syntax tree shown in listing \ref{lst:SyntaxTreeEx}. Each class of the syntax tree implements the \texttt{SyntaxTreeNode} abstract class. For the visitor class the \texttt{SyntaxTreeVisitor} interface needs to be implemented. Both classes are using generics. This allows the implementation of the visitor to use an arbitrary type as a return value. Multiple visitors can then be implemented using the generics, so that each visitor can return one type of the AST types. In this case it is helpful to create an abstract base visitor that provides an empty implementation for all interface's methods. Then the concrete visitor only needs to override the methods that are relevant for the specific AST type.  

A \texttt{SyntaxTreeNode} can then be visited by calling its \texttt{accept} method. Inside the \texttt{accept} method, the appropriate method of the \texttt{SyntaxTreeVisitor} will be called. As can be seen in listing \ref{lst:ListPatNumbNode} the \texttt{NumberNode} calls the \texttt{visitNumberNode} with itself as the parameter. This behavior is analogous to all other nodes of the syntax tree. 


\begin{KotlinCode}[float,numbers=none,caption=Interface and abstract class used to implement the visitor-pattern., label=lst:ListPatIntEx]
sealed class SyntaxTreeNode {
    abstract fun <T> accept(visitor: SyntaxTreeVisitor<T>): T
}

interface SyntaxTreeVisitor<T> {
    fun visitNumberNode(node: NumberNode): T
    fun visitOperatorNode(node: OperatorNode): T
}
\end{KotlinCode}


\begin{KotlinCode}[float,numbers=none,caption=Implementation of the \texttt{NumberNode} class inheriting from the \texttt{SyntaxTreeNode}., label=lst:ListPatNumbNode]
  data class NumberNode(val value: Int) : SyntaxTreeNode() {
    override fun <T> accept(visitor: SyntaxTreeVisitor<T>): <T> {
        return visitor.visitNumberNode(this)
    }
}
  \end{KotlinCode}


\subsection{Listener-Pattern for Tree Transformation}

The listener-pattern is used for \textit{listening} to events or notifications from another object. In the context of parsing, the listener-pattern is used to handle parse events coming from the parser. This includes events such as entering and exiting a node during the parse. When using the listener-pattern the parse tree is only traversed once. This is because the events get pushed to the listeners during the parse. 

To implement the listener-pattern for the construction of an AST, a listener interface is needed. The listener interface contains methods declarations for entering and exiting each node type. The methods take the syntax tree node as the input parameter. A possible implementation for the listener interface is shown in.  In case of the \textit{enter} methods, the syntax tree node has not been parsed yet, so no data from it is available yet.

A concrete listener will then implement the interface and register/subscribe itself to the events of the parser. When the parser enters or exits a node during parse it will call the respective method with the parsed syntax tree node for all listeners. 

\begin{KotlinCode}[float,numbers=none,caption=Implementation of the \texttt{ExpressionListener} interface., label=lst:ListPatExprList]
interface ExpressionListener {
    fun enterExpr(node: Expression)

    fun exitExpr(node: Expression)

    fun enterNumer(number: Number)

    fun exitNumber(number: Number)
}
  \end{KotlinCode}

