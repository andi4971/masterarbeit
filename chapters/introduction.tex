\chapter{Introduction}
\label{cha:introduction}

\section{Motivation}

Compilers function as the backbone for computer programming. A compiler takes care of translating human-readable source code into something a computer can understand. This allows the application developer to focus only on writing the application, without having to worry about the technicalities of the concrete computer where the software will run on. For one programming language there may exist multiple compilers targeting different kinds of computers. This allows the same source code to run for example on Linux and Windows. This flexibility saves the developer a lot of work, because they don't need to rewrite their application in the case they also want to target another operating system. Furthermore, there also exist compilers that target virtual machines like the Java Virtual Machine (JVM). Generating code for a virtual machine has the advantage that there doesn't need to be compilers written for every target operating system. Instead, for each operating system an implementation of the virtual machine is provided.

The process of compiling source code begins in the frontend of the compiler. In this step the frontend reads the source code, and constructs an abstract syntax tree (AST). The AST is a runtime representation of the source code in memory. It contains only the necessary information that is later on needed to generate machine code. The process of constructing the AST is based on the grammar of the programming language. Based on this grammar a lexer and parser are either written manually or get generated by a parser generator tool like ANTLR. In the case of ANTLR the generated parser and lexer construct a full parse tree from the input. From the parse tree an AST can be constructed using for example the visitor-pattern. 

The AST functions then as the input for the backend of the compiler. In this section the machine code for the target system is generated. In the case of the JVM this is the so called bytecode. The bytecode could be written by hand, however this is rather difficult. For this a detailed understanding of the instruction set is needed, and the actual generation would have to be performed on a byte array. Therefore, APIs exist that provide an abstraction layer to the code generation. One API for bytecode generation is the open source project ObjectWeb ASM or just ASM. It provides an API that utilizes the visitor-pattern to generate bytecode instructions. 


\section{Task and Goal}

MiniC++ is a subset of the C++ programming language. The scope of MiniC++ is very limited in comparison to C++. This cut down version of C++ is used at the university for teaching software engineering master students about compilers in the formal languages class. In this class all aspects of a compiler are discussed. First the principles of lexers and parsers are explained. Then the concepts of syntax trees and further abstract syntax trees are introduced. Finally, code generation is explained. 

In the exercises, students use a MiniC++ compiler to compile MiniC++ source code to .Net Common Intermediate Language (CIL). The frontend of the compiler is generated by using the compiler generator Coco-2. Coco-2 includes both, the lexer and the parser. There is only one grammar-file required for the definition of the lexer and parser. Furthermore, semantic actions can also be included to create an attributed grammar (ATG). 

In this master thesis a compiler for MiniC++ will be created. The compiler will be built upon Java technologies. Output of the compiler will be Java bytecode that can be executed on the Java Virtual Machine (JVM). The frontend is based on a lexer and parser generated by the parser generator ANTLR\footnote{https://www.antlr.org/} (ANother Tool for Language Recognition). ANTLR is used to generate a full syntax tree. From this syntax tree an abstract syntax tree (AST) is constructed. The backend utilizes the ObjectWeb ASM\footnote{https://asm.ow2.io/} library. This library provides an API to generate Java bytecode. 

This master thesis will further explore the capabilities of ANTLR. ANTLR provides multiple ways to interact with the generated parser. The master thesis compares the advantages and disadvantages of each of the options.

\section{Theoretical Fundamentals}

This section explains the basic concepts behind formal languages and how they are used in compilers. Further, the individual components of a compiler are highlighted. 

\subsection{Formal Languages and Compiler}

Formal languages make up the fundament on which compilers are built upon. In comparison to natural languages, formal languages have a fixed grammar. This grammar does not evolve naturally, as it does with natural languages. A formal grammar is defined by replacement rules. A replacement rule defines that a symbol \textit{A} can be replaced by a chain $\alpha$. The chain may contain terminal and non-terminal symbols. 

Grammars can be classified according to the Chomsky hierarchy \parencite{CHOMSKY1959137}. Chomsky classifies Grammars into four categories. Of those the first two are relevant for compiler construction. Namely, regular grammars and context-free grammars. The four categories are differentiated by the type of rules that can be defined. The types of rules used then define which kind of automaton is needed to recognize sentences of the given language. 

\subsubsection{Regular Grammars}

Regular grammars make up the simplest group of grammars. For a grammar to be a regular grammar all rules must be in the form of $A\rightarrow a | B$. This means that a non-terminal symbol $A$ can only be replaced by either a terminal symbol $a$ or another non-terminal symbol $B$. The only exception is the root rule $S$ which can be replaced by the empty chain. 

To recognize a sentence of a regular grammar a deterministic finite automaton (DFA) is needed. A DFA consists of the following elements:

\begin{itemize}
    \item $S$ finite non-empty set of states
    \item $\Sigma$ finite non-empty set of symbols (Alphabet)
    \item $s_0$ initial state, $s_0 \in S$
    \item $\delta$ state transition function, $S \times \Sigma \rightarrow S$
    \item $F$ set of final states, $F \subset S$
\end{itemize}

The DFA proceeds to read the symbols in $\Sigma$ one symbol at a time. The current symbol is then used in combination with the current state in the state transition function to acquire a new state. This process is continued until a final state is reached, meaning that a sentence has successfully been recognized. In case that for the current symbol and state no entry in the state transition function can be found, the recognition has failed, and the given input is not a sentence of the language. 

A DFA can be implemented in a program to efficiently detect sentences of a language. For more complicated regular grammars a nondeterministic finite automaton (NFA) is easier to construct. A NFA program however is more complicated and slower compared to a DFA one. Every NFA can be transformed into a DFA to overcome this limitation. After transformation the constructed DFA may have more than the minimal amount of states needed. A second transformation can be performed that reduces the DFA to a minimal DFA. 

\subsubsection{Context-Free Grammars}

Context-free grammars are the second group of grammars according to the Chomsky hierarchy. Context-free grammars also include regular grammars, meaning that every regular grammar is also a context-free grammar. A replacement in rule of a context-free grammar is in the form $A \rightarrow \beta$. Meaning that a non-terminal symbol $A$ can be replaced by a chain $\beta$ containing terminal and non-terminal symbols or also $\epsilon$, the empty chain. 

In a context-free grammar central recursion is possible (direct or indirect). This enables the nested structures that are needed for programming languages e.g. for expression hierarchies. Central recursion cannot be detected by a DFA, for this a pushdown automaton is needed. With a deterministic pushdown automaton (DPDA) all deterministic context-free grammars can be recognized. To recognize all context-free grammars a nondeterministic pushdown automaton is needed. For programming languages deterministic context-free grammars are used. 

There are two strategies for constructing a syntax tree from a sentence of a context-free grammar, namely top-down and bottom-up. Which strategy can be used depends on the kind of deterministic context-free grammar that is used. Following are the two most important conditions for context-free grammars:

\begin{itemize}
    \item \textbf{LL($k$)-Condition:} Defines how many $k$ symbols look ahead while parsing are needed to deterministically decide on the next rule when using the \textit{top-down} strategy.
    \item \textbf{LR($k$)-Condition:} Defines how many $k$ symbols look ahead while parsing are needed to deterministically decide on the next rule when using the \textit{bottom-up} strategy.
\end{itemize}

The higher $k$ the more complicated parsing becomes. Therefore, LL($1$) and LR($1$) grammars are preferred. For an LL($1$) or LR($1$) grammar only one look ahead symbol is needed to deterministically decide on the next rule. 

LL($k$) grammars can be recognized with a normal DPDA. For LL($1$) grammars it is also feasible to implement an efficient recursive descent parser. In the case of an LR($1$) grammar, the DPDA must be extended to be able to read an arbitrary amount of symbols at the same time. Only then is it able to recognize a sentence of an LR($1$) grammar with the \textit{bottom-up} strategy. It has to be noted that a DPDA able of LR($k$) grammars, is also able to recognize all LL($k$) grammars.

\subsection{Compiler Construction}

The task of a compiler is to translate code of a given source language into code of a target language. The source language being a human-readable programming language like Java and the target language being machine code for a given operating system and architecture, or also a virtual machine. Compiling code involves the following steps: 
  
\begin{itemize}
    \item Lexical Analysis
    \item Syntactic Analysis
    \item Semantic Evaluation
    \item Intermediate Language Generation
    \item Optimization
    \item Code Generation
\end{itemize}


%\subsubsection{Lexical Analysis}
The lexical analysis is the first step of the compilation. It reads the source code and classifies it. The goal is to group the individual characters into symbols. The grammar of the source language provides the information about the symbols. This part of the grammar is defined within the constraints of a regular grammar. 

The symbols can be divided into terminal symbols and terminal classes. Terminal symbols are the keywords of the source language, e.g. \texttt{int break function}. Terminal classes are for example all numbers or identifiers. Comments are also handled at this step. Since comments usually have no influence on the generated code, they are removed. All recognized symbols are then passed on to the parser (syntactic analysis and semantic evaluation). 


%\subsubsection{Syntactic Analysis}

The syntactic analysis takes the terminal symbols and classes recognized in the lexical analysis as input to construct the syntax tree. A context-free grammar provides the basis for the syntax tree. During the syntactic analysis the terminal symbols are grouped into syntactic elements according to the grammar. Furthermore, the syntactic integrity is also checked. In case there is no grammar rule available for the current terminal symbol, the syntactic analysis has failed, and a syntax error is reported.

%\subsubsection{Semantic Evaluation}

During the syntactic analysis the semantic evaluation is performed. This includes constructing the abstract syntax tree (AST). In the AST only the relevant information for the code generation is contained. For each rule in the grammar there may be a semantic action associated with it, that gets executed when the rule is visited. The semantic action has access to the attributes of the rule. This information is used to generate the AST.  

Taking the AST as a basis, the intermediate language code generation is performed. This includes for example generating the symbol table. 

Afterward, the intermediate language code is analyzed and optimized. This may include optimizations such as inlining or loop unrolling. Depending on the use case more aggressive optimizations can also be performed. 

Finally, the code generation unit takes the optimized code and generates the appropriate instructions for the target language. 