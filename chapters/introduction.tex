\chapter{Introduction}
\label{cha:introduction}

\section{Motivation}

Compilers function as the backbone for computer programming. A compiler takes care of translating human-readable source code into something a computer can execute. This allows the application developers to focus on writing the application, without having to worry about the technicalities of the concrete computer where the software will run on. For one programming language there may exist multiple compilers targeting different kinds of computers. This allows the same source code to run for example on Linux and Windows computers with Intel or ARM processors. This flexibility saves developers a lot of work, because they don't need to rewrite their application in the case they also want to target another operating system and/or processor. Furthermore, there exist compilers that target virtual machines like the Java Virtual Machine (JVM). Generating code for a virtual machine has the advantage that there is no need for compilers for every target operating system and/or processor. Instead, for each operating system an implementation of the virtual machine is provided.

The process of compiling source code begins in the frontend of the compiler. The frontend reads the source code and constructs an abstract syntax tree (AST). The AST is a runtime representation of the source code in memory. It contains only the necessary information that is later on needed to generate target code. The process of constructing the AST is based on the grammar of the programming language. Based on this grammar a lexer and parser are either written manually or get generated by a parser generator tool like ANTLR. In the case of ANTLR the generated lexer and parser construct a full parse tree from the input. From the parse tree an AST can be constructed using for example the visitor-pattern. 

The AST functions then as the input for the backend of the compiler: The backend generates code for the target system. In the case of the JVM this is the so called bytecode. 
%The bytecode could be written by hand, however this is rather difficult. For this a detailed understanding of the instruction set is needed, and the actual generation would have to be performed on a byte array.
 APIs exist that provide an abstraction layer to the code generation. One API for bytecode generation is the open source project ObjectWeb ASM or just ASM \parencite{bruneton2007asm}. It provides an API that utilizes the visitor-pattern to generate bytecode instructions. 


\section{Task and Goal}

MiniC++ is a subset of the C++ programming language. The scope of MiniC++ is very limited in comparison to C++. It is used at the University of Applied Sciences Upper Austria for teaching software engineering master students about compilers in the formal languages class. In this class, all aspects of a compiler are discussed. First, the principles of lexers and parsers are explained. Then the concepts of syntax trees and further abstract syntax trees are introduced. Finally, code generation is explained. 

In the exercises, students use a MiniC++ compiler to compile MiniC++ source code to .NET Common Intermediate Language (CIL). The frontend of the compiler is generated by using the compiler generator Coco-2 \parencite{doblerCoco2}. Which generates both, the lexer and the parser. There is only one input-file required for the definition of the lexer and the parser. Furthermore, attributes and semantic actions can be included to create an attributed grammar (ATG). 

In this master thesis, a compiler for MiniC++ will be created. The compiler will be built upon Java technologies. Output of the compiler will be Java bytecode that can be executed on the Java Virtual Machine (JVM). The frontend is based on a lexer and parser generated by the parser generator ANTLR\footnote{https://www.antlr.org/} (ANother Tool for Language Recognition). They are used to generate a full syntax tree. From this syntax tree an abstract syntax tree (AST) is constructed. The backend utilizes the ObjectWeb ASM\footnote{https://asm.ow2.io/} library. This library provides an API to generate Java bytecode. 

This master thesis will further explore the capabilities of ANTLR. ANTLR provides multiple ways to interact with the generated parser. The master thesis compares the advantages and disadvantages of each of these options.

\section{Theoretical Fundamentals}

This section explains the basic concepts behind formal languages and how they are used in compilers. Furthermore, the individual components of a compiler are highlighted. 

\subsection{Formal Languages and Compilers}

Formal languages make up the fundament on which compilers are built upon. In comparison to natural languages, formal languages have a syntax which can be defined by a grammar. This grammar does not evolve naturally, as it does with natural languages. A formal grammar is defined by replacement rules. A replacement rule defines that a non-terminal symbol \textit{A} can be replaced by a sequence $\alpha$. The sequence may contain terminal and non-terminal symbols. 

Grammars can be classified according to the Chomsky hierarchy \parencite{CHOMSKY1959137}. Chomsky classifies formal languages and their grammars into four categories. Of those, the first two are relevant for compiler construction. Namely, regular grammars and context-free grammars. The four categories are differentiated by the type of rules that can be defined. The types of rules used then define which kind of automaton is needed to recognize sentences of the given language. 

\subsubsection{Regular Grammars}

Regular grammars make up the simplest group of grammars. For a grammar to be a regular grammar all rules must be in the form of $A\rightarrow a | a B$. This means that a non-terminal symbol $A$ can only be replaced by either a terminal symbol $a$ or a terminal symbol $a$ followed by a non-terminal symbol $B$. The only exception is the root rule $S$ which can be replaced by the empty sequence. 

To recognize a sentence of a regular grammar a finite automaton (FA) can be used. A deterministic FA consists of the following elements:

\begin{itemize}
    \item $S$ finite, non-empty set of states
    \item $\Sigma$ finite, non-empty set of symbols (alphabet)
    \item $s_0$ initial state, $s_0 \in S$
    \item $\delta$ state transition function, $S \times \Sigma \rightarrow S$
    \item $F$ set of final states, $F \subseteq S$
\end{itemize}

The DFA proceeds to read the symbols in $\sigma$ one symbol at a time. The current symbol is then used in combination with the current state in the state transition function to acquire a new state. This process is continued until a final state is reached, meaning that a sentence has successfully been recognized. In case that for the current symbol and state no entry in the state transition function can be found, the recognition failed, and the given input is not a sentence of the language. 

A DFA can be implemented in a program to efficiently detect sentences of a language. For more complicated regular grammars a nondeterministic finite automaton (NFA) is easier to construct. A NFA program however is more complicated and slower compared to a DFA one. Every NFA can be transformed into a DFA to overcome this limitation. After transformation the constructed DFA may have more than the minimal amount of states needed. A second transformation can be performed that reduces the DFA to a minimal DFA. 

\subsubsection{Context-Free Grammars}

Context-free grammars are the second group of grammars according to the Chomsky hierarchy. Context-free grammars also include regular grammars, meaning that every regular grammar is also a context-free grammar. A replacement rule of a context-free grammar is in the form $A \rightarrow \beta$. Meaning that a non-terminal symbol $A$ can be replaced by a sequence $\beta$ containing terminal and non-terminal symbols or also $\epsilon$, the empty sequence. 

In a context-free grammar central recursion is possible (direct or indirect). This allows the nested structures that are needed for programming languages, e.g., for expression hierarchies. Central recursion cannot be handled by a DFA, for this a pushdown automaton is needed. With a deterministic pushdown automaton (DPDA) all deterministic context-free grammars can be recognized. To recognize all context-free grammars a nondeterministic pushdown automaton is needed. For programming languages deterministic context-free grammars are used. 

There are two strategies for constructing a syntax tree from a sentence of a context-free grammar, namely top-down and bottom-up. Which strategy can be used depends on the kind of deterministic context-free grammar that is used. Following are the two most important conditions for context-free grammars:

\begin{itemize}
    \item \textbf{LL($k$) Condition:} Defines that a maximum of $(k)$ symbols look ahead while parsing are sufficient to deterministically decide on the next rule when using the \textit{top-down} strategy.
    \item \textbf{LR($k$) Condition:} Defines that a maximum of $(k)$ look ahead while parsing are sufficient to deterministically decide on the next rule when using the \textit{bottom-up} strategy.
\end{itemize}

The higher the value of $k$, the more complicated parsing becomes. Therefore, LL($1$) and LR($1$) grammars are preferred. For an LL($1$) or LR($1$) grammar only one look ahead symbol is needed to deterministically decide on the next rule. 

LL($k$) grammars can be recognized with a normal DPDA. For LL($1$) grammars it is also feasible to implement an efficient recursive descent parser. In the case of an LR($1$) grammar, the DPDA must be extended to be able to use an arbitrary amount of symbols at stack. Only then is it able to recognize a sentence of an LR($1$) grammar with the \textit{bottom-up} strategy. It has to be noted that a DPDA which is able to recognize LR($k$) grammars, is also able to recognize LL($k$) grammars.

\subsection{Compiler Construction}

The task of a compiler is to translate code of a given source language into code of a target language. The source language being a human-readable programming language like Java and the target language being code for a given operating system and processor architecture, or a virtual machine. Compiling code can be separated into two main stages: frontend and backend.
The frontend consists of the following steps:
\begin{itemize}
    \item lexical analysis
    \item syntactic analysis
    \item semantic evaluation
    \item intermediate language generation
\end{itemize}

The backend performs optimization and code generation.

%\subsubsection{Lexical Analysis}
The lexical analysis is the first step of the compilation. It reads the source code and organizes it. The goal is to group individual characters into symbols and to skip meaningless characters (e.g. comments). The grammar of the source language provides the information about the symbols. This part of the grammar is defined using a regular grammar. 

The symbols can be divided into terminal symbols and terminal classes. Terminal symbols are special symbols like \texttt{=}, \texttt{(} \texttt{-} and the keywords of the source language, e.g. \texttt{int}, \texttt{break},  \texttt{function}. Terminal classes are for example all numbers or identifiers. Comments are also handled at this step. Since comments usually have no influence on the generated code, they are removed. All recognized symbols are then passed on to the parser (syntactic analysis and semantic evaluation). 


%\subsubsection{Syntactic Analysis}

The syntactic analysis takes the terminal symbols and classes recognized in the lexical analysis phase as input to construct the syntax tree. A context-free grammar provides the basis for the syntax tree. During the syntactic analysis the terminal symbols are grouped into syntactic elements according to the grammar. Furthermore, the syntactic integrity is also checked. In case there is no grammar rule available for the current terminal symbol, the syntactic analysis has failed, and a syntax error is reported.

%\subsubsection{Semantic Evaluation}

According to the principle of syntax-directed parsing, during the syntactic analysis the semantic evaluation is performed. This may include constructing the abstract syntax tree (AST). In the AST only the relevant information for the code generation is contained. For each rule in the grammar, there may be semantic actions associated with it, that get executed when the rule is visited. The semantic action has access to the attributes of the rule. This information is used to generate the AST.  

%Taking the AST as a basis, the intermediate language code generation is performed. This includes for example generating the symbol table. 

Afterwards, the intermediate language code is analyzed and optimized. This may include optimizations such as inlining or loop unrolling. Depending on the use case more aggressive optimizations can also be performed. 

Finally, the code generation unit takes the optimized code and generates the appropriate instructions for the target language. 